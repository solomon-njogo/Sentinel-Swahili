# Data Pipeline Dependencies
# Core libraries for data processing and analysis

# Note: Standard library modules used:
# - os, re, json, logging, collections, typing, datetime, argparse
# These are all part of Python's standard library, no external dependencies required.

# Text Cleaning Dependencies
# NLTK is optional - the system will use fallback implementations if NLTK is not available
nltk>=3.8  # Natural Language Toolkit (optional, for enhanced stop-word and stemming support)

# Note: After installing NLTK, you may need to download stopwords data:
# python -c "import nltk; nltk.download('stopwords')"
# However, the system works without NLTK using custom Swahili stop-word lists and stemmers.

# Dataset Structuring Dependencies
numpy>=1.23.0  # Required for feature-target split and numerical operations

# Transformer Fine-tuning Dependencies
transformers>=4.30.0  # Hugging Face transformers for tokenizers and model utilities
torch>=2.0.0  # PyTorch for deep learning
peft>=0.4.0  # Parameter-Efficient Fine-Tuning (LoRA support)
datasets>=2.12.0  # Hugging Face datasets library for advanced data handling
accelerate>=0.20.0  # Hugging Face accelerate for distributed training and optimization
scikit-learn>=1.3.0  # For evaluation metrics (accuracy, F1, precision, recall)

# EDA and Visualization Dependencies
matplotlib>=3.6.0  # For visualizations and plotting
seaborn>=0.12.0  # Statistical data visualization
langdetect>=1.0.9  # Language detection
gensim>=4.3.0  # Topic modeling (LDA)

# Optional dependencies for future enhancements:
# pandas>=1.5.0  # For advanced data manipulation (if needed)

